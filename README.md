# Sequence to Sequence Models

In this task, we implement a sequence-to-sequence model for the task of replicating input sequences. The idea is to create a compressed representation of input sequence using an encoder RNN, and use this hidden state and decoder-RNN to generate the input sequence!

## Dependencies

  1. tensorflow==1.0.1
  2. numpy==1.13.0
  3. matplotlib==1.5.1

**Note- Tensorflow has made a lot of changes to different module location post release 1.0+, so ensure you have the correct version installed!

## Model Architecture

A general sequence-2-sequence model architecture can be seen in the image below. The model has been used for training a chatbot, but theidea behind training any seq2seq model is same:

![Image](https://github.com/darshanbagul/Seq2Seq_tensorflow/blob/master/images/seq2seq2_architecture.png)
Image Credits: [WildML blog](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)

Note the thought vector in the figure above. This is the hidden embedded representation of the input sequence generated by an encoder network, which serves as input to the decoder network. In a sense, the network is able to capture the context and meaning from the input sequence in the thought vector.

## Training

Below, we can see a graph of minibatch loss vs epochs. I trained over 5000 epochs with a batch size of 1000. 
I used Adam for optimization.

![Image](https://github.com/darshanbagul/Seq2Seq_tensorflow/blob/master/images/train_loss.png)

## Results

Below, we can see how the model is able to replicate the input sequences efficiently as it trains across 5000 epochs. The model will overfit in order to replicate the exact input sequences.

 1. batch 0
   
   minibatch loss: 2.38790130615
   
   sample 1:
   
     input     > 2 2 3 2 3 5 3 0
     
     predicted > 2 2 1 9 2 9 9 9 2 9 0
   
   sample 2:
     
     input     > 2 5 8 0 0 0 0 0
     
     predicted > 2 2 1 9 9 9 0 0 0 0 0
   
   sample 3:
     
     input     > 7 7 5 3 3 6 0 0
     
     predicted > 7 7 7 7 7 7 7 7 7 0 0
     
 2. batch 3000
  
  minibatch loss: 0.0798834934831
  
  sample 1:
  
    input     > 4 4 9 7 5 0 0 0
    
    predicted > 4 4 9 7 5 1 0 0 0 0 0
  
  sample 2:
    
    input     > 7 3 8 2 9 0 0 0
    
    predicted > 7 3 8 2 9 1 0 0 0 0 0
  
  sample 3:
    
    input     > 3 5 2 5 4 9 3 7
    
    predicted > 5 3 2 5 4 9 3 7 1 0 0
    
 3. batch 5000
  
  minibatch loss: 0.0286562200636
  
  sample 1:
    
    input     > 4 8 4 5 3 4 3 9
    
    predicted > 4 8 4 5 3 4 3 9 1 0 0
  
  sample 2:
    
    input     > 9 5 7 7 5 4 0 0
    
    predicted > 9 5 7 7 5 4 1 0 0 0 0
  
  sample 3:
    
    input     > 3 6 6 5 2 0 0 0
    
    predicted > 3 6 6 5 2 1 0 0 0 0 0
    
## Credits
The implementation of the model is based on research published in the paper Sequence to Sequence Learning
with Neural Networks (https://arxiv.org/abs/1409.3215) at NIPS-2014.

A big shout out to Siraj Raval (@sirajology) for teaching an amazing course on Deep Learning at Udacity. 

I find these resources extremely useful:
 1. [WildMl blog](http://www.wildml.com/) by Denny Britz
 2. Christopher Olah's [blog](colah.github.io/posts/2015-08-Understanding-LSTMs/)
 
