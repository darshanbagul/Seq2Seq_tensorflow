{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Models\n",
    "\n",
    "Sequence to sequence models comprise of **two recurrent neural networks (RNNs)**: an **encoder** that processes the input and a **decoder** that generates the output.\n",
    "\n",
    "The task of encoder is to generate an efficient representation of input sequence which be used by the decoder network for generating the output in context that we train the decoder in. \n",
    "\n",
    "Consider the task of language translation or something complex such as captioning of images. The task of encoder is to learn structures in the input data (text/images) and generate an n-dimensional representation of the input data, which we refer to as the hidden state.\n",
    "The hidden state vector is then fed to a decoder network to generate output sequences. These are very powerful model which have been deployed for tasks such as:\n",
    "    1. Neural Machine Translation\n",
    "    2. Image captioning\n",
    "    3. Generating subtitles for songs/audio\n",
    "    4. Image generation from text! etc.\n",
    "    \n",
    "The encoder network can also be seen as an embedding network, which reduces the dimensionality of data and embeds the input sequences into n dimensions. These networks can be used for sequential data compression, where we can store humongous amounts of data as n-dimensional vectors. The weights of the encoder-RNN and decoder-RNN can be stored seperately along with the network configuration. We can expand the compressed data using the decoder network. Just imagine terrabytes of audio data being stored in just n-dimensional vectors!\n",
    "\n",
    "In this task, we implement a sequence-to-sequence model for the task of replicating input sequences. The idea is to create a compressed representation of input sequence using an encoder RNN, and use this hidden state and decoder-RNN to generate the input sequence!\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np #matrix math \n",
    "import tensorflow as tf #machine learningt\n",
    "import helpers #for formatting data into batches and generating random sequence data\n",
    "\n",
    "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "sess = tf.InteractiveSession() #initializes a tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20 #character length\n",
    "\n",
    "encoder_hidden_units = 20 #num neurons\n",
    "decoder_hidden_units = encoder_hidden_units * 2 #in original paper, they used same number of neurons for both encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
    "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "#this thing could get huge in a real world application\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnn'):\n",
    "    ((encoder_fw_outputs,\n",
    "      encoder_bw_outputs),\n",
    "     (encoder_fw_final_state,\n",
    "      encoder_bw_final_state)) = (tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded,\n",
    "                                        sequence_length=encoder_inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'rnn/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'rnn/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'rnn/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
    "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "#bias\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    #end of sentence\n",
    "    initial_input = eos_step_embedded\n",
    "    #last time steps cell state\n",
    "    initial_cell_state = encoder_final_state\n",
    "    #none\n",
    "    initial_cell_output = None\n",
    "    #none\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    \n",
    "    def get_next_input():\n",
    "        #dot product between previous ouput and weights, then + biases\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        #Logits simply means that the function operates on the unscaled output of \n",
    "        #earlier layers and that the relative scale to understand the units is linear. \n",
    "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
    "        #(you might have an input of 5).\n",
    "        #prediction value at current time step\n",
    "        \n",
    "        #Returns the index with the largest value across axes of a tensor.\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        #embed prediction for the next input\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    \n",
    "    \n",
    "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    \n",
    "    #set previous to current\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
    "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
    "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
    "#and what to emit for the output.\n",
    "#ta = tensor array\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "#flettened output tensor\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "#pass flattened tensor through decoder\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "#prediction vals\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "#loss function\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "#train it \n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[7, 9, 6, 9, 9]\n",
      "[6, 6, 5, 4, 6, 2, 7]\n",
      "[9, 5, 2]\n",
      "[2, 6, 6, 4, 2]\n",
      "[5, 5, 3]\n",
      "[9, 9, 7, 2, 2]\n",
      "[7, 5, 3, 6]\n",
      "[5, 2, 4, 3]\n",
      "[3, 5, 9, 4, 5]\n",
      "[4, 2, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.38790130615\n",
      "  sample 1:\n",
      "    input     > [2 2 3 2 3 5 3 0]\n",
      "    predicted > [2 2 1 9 2 9 9 9 2 9 0]\n",
      "  sample 2:\n",
      "    input     > [2 5 8 0 0 0 0 0]\n",
      "    predicted > [2 2 1 9 9 9 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 7 5 3 3 6 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 7 7 0 0]\n",
      "()\n",
      "batch 1000\n",
      "  minibatch loss: 0.481568276882\n",
      "  sample 1:\n",
      "    input     > [6 6 8 9 0 0 0 0]\n",
      "    predicted > [6 8 6 9 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 9 2 3 5 0 0 0]\n",
      "    predicted > [3 9 2 3 5 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 9 2 2 2 5 2 0]\n",
      "    predicted > [5 5 2 2 2 2 2 1 0 0 0]\n",
      "()\n",
      "batch 2000\n",
      "  minibatch loss: 0.206333577633\n",
      "  sample 1:\n",
      "    input     > [3 8 7 3 9 4 9 0]\n",
      "    predicted > [3 8 7 3 9 9 9 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 5 7 4 9 4 0 0]\n",
      "    predicted > [4 5 7 4 9 4 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 4 2 3 0 0 0 0]\n",
      "    predicted > [4 4 2 3 1 0 0 0 0 0 0]\n",
      "()\n",
      "batch 3000\n",
      "  minibatch loss: 0.0798834934831\n",
      "  sample 1:\n",
      "    input     > [4 4 9 7 5 0 0 0]\n",
      "    predicted > [4 4 9 7 5 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 3 8 2 9 0 0 0]\n",
      "    predicted > [7 3 8 2 9 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 5 2 5 4 9 3 7]\n",
      "    predicted > [5 3 2 5 4 9 3 7 1 0 0]\n",
      "()\n",
      "batch 4000\n",
      "  minibatch loss: 0.0961628556252\n",
      "  sample 1:\n",
      "    input     > [7 7 6 0 0 0 0 0]\n",
      "    predicted > [7 7 6 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 3 6 2 2 0 0 0]\n",
      "    predicted > [8 3 6 2 2 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 9 2 0 0 0 0 0]\n",
      "    predicted > [9 9 2 1 0 0 0 0 0 0 0]\n",
      "()\n",
      "batch 5000\n",
      "  minibatch loss: 0.0286562200636\n",
      "  sample 1:\n",
      "    input     > [4 8 4 5 3 4 3 9]\n",
      "    predicted > [4 8 4 5 3 4 3 9 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 5 7 7 5 4 0 0]\n",
      "    predicted > [9 5 7 7 5 4 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 6 6 5 2 0 0 0]\n",
      "    predicted > [3 6 6 5 2 1 0 0 0 0 0]\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "max_batches = 5001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0292 after 500100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naAV9WaGtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c3a6090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "As the graph of training loss suggests, the network trains as we expect. Notice the output sequences generated as the number of epochs over mini-batches increases. The ability of the model to generate exact input sequences grows as the model trains over each epoch; just as we predicted!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
